{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b02316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\stephenprior\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:175: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "from transformer_utils.low_memory import enable_low_memory_load\n",
    "import transformers\n",
    "\n",
    "enable_low_memory_load() #without this model ends up on CPU?\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d045912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' flooding'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.visualization import visualize\n",
    "text = \"The forecast said that thunderstorm activity with the risk of hail will lead to localised\"\n",
    "tokens = visualize.text_to_input_ids(tokenizer, text)\n",
    "expected_token_id = 17448\n",
    "tokenizer.decode(expected_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df353c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoBlock(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPTNeoAttention(\n",
       "    (attention): GPTNeoLocalSelfAttention(\n",
       "      (attn_dropout): Dropout(p=0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0, inplace=False)\n",
       "      (k_proj): Linear(in_features=0, out_features=768, bias=False)\n",
       "      (v_proj): Linear(in_features=0, out_features=768, bias=False)\n",
       "      (q_proj): Linear(in_features=0, out_features=768, bias=False)\n",
       "      (out_proj): Linear(in_features=0, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPTNeoMLP(\n",
       "    (c_fc): Linear(in_features=0, out_features=3072, bias=True)\n",
       "    (c_proj): Linear(in_features=0, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetLayer = model.transformer.h[11]\n",
    "targetLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5365ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_embeds(embeds):\n",
    "    transformer_outputs = model.transformer.forward(inputs_embeds=embeds)\n",
    "    hidden_states = transformer_outputs[0]\n",
    "    lm_logits = model.lm_head(hidden_states)\n",
    "    return lm_logits[0,-1,expected_token_id:expected_token_id+1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5ed74",
   "metadata": {},
   "source": [
    "Looping through transformer layers starting closest to the output, it can be seen how influence spreads.\n",
    "\n",
    "Close to the output attribution is almost entirely against the current position => lower layers.\n",
    "\n",
    "Different positions become important and unimportant at different lower layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c30269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerConductance\n",
    "labels = [tokenizer.decode(token) for token in tokens[0]]\n",
    "\n",
    "embeds = model.transformer.wte(tokens)\n",
    "lc=LayerConductance(forward_with_embeds, targetLayer)\n",
    "embeds.requires_grad_()\n",
    "attr, delta = lc.attribute(embeds,return_convergence_delta=True, attribute_to_layer_input=True)#, target=expected_token_id)\n",
    "#for i in range(11,0,-1):   \n",
    "#    plt = visualize.plot_layer_attribution(model, tokens, labels, lc)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1056171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.0469e-06, -1.7967e-05,  1.8727e-06,  ...,  1.6404e-07,\n",
       "           1.4560e-05,  3.4223e-07],\n",
       "         [-4.9220e-11, -4.8162e-10,  2.4908e-10,  ..., -1.2356e-10,\n",
       "          -3.9437e-10,  1.7395e-10],\n",
       "         [-2.4175e-11, -2.3006e-10, -1.3076e-10,  ..., -2.2518e-11,\n",
       "          -4.3285e-11,  9.3857e-11],\n",
       "         ...,\n",
       "         [-2.1397e-03,  5.9094e-03,  7.8610e-04,  ...,  5.2313e-03,\n",
       "          -5.0027e-03,  1.1392e-02],\n",
       "         [-1.1459e-02, -1.7288e-04, -3.9267e-02,  ..., -2.1111e-03,\n",
       "          -1.3900e-03,  4.9637e-03],\n",
       "         [ 3.6016e-01, -8.1744e-02, -2.2772e-02,  ..., -4.6252e-02,\n",
       "          -1.4187e-01,  4.7693e-03]]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90379af",
   "metadata": {},
   "source": [
    "These \"waveforms\" show the attribution of single vectors input to each transformer layer (in all cases attribution is to the predicted next token). The suspicion was that these might stack - certain neurons would become important in different layers, but would remain important to the top - but this doesn't appear to be the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00d2b2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1a7a7ce1fec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, target=expected_token_id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(11,0,-1):\n",
    "    lc = LayerConductance(forward_with_embeds, model.transformer.h[i])\n",
    "    embeds = model.transformer.wte(tokens)    \n",
    "    embeds.requires_grad_()\n",
    "    attr = lc.attribute(embeds, attribute_to_layer_input=True)#, target=expected_token_id)\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "    plt.plot(attr[0,16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4bb02c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
